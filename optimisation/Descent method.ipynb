{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1899ced",
   "metadata": {},
   "source": [
    "### Descent methods\n",
    "\n",
    "The notebook about approximate line search goes into some of the basics, might be useful before expaning on this more.\n",
    "\n",
    "A descent method uses iterations, and steps of progressively shorter values, to minimise (or maximise) decision variables. Eg in $ \\mathbb{R}^2 $ you will have one independent and one dependent variable, so this looks like a standard function. In $ \\mathbb{R}^3 $ you will have two independent variables.\n",
    "\n",
    "Then the descent method given 1 independent variable will look like this:\n",
    "\n",
    "$ \\{ x_n = x_1, x_2, x_3, ... x_n \\} $\n",
    "\n",
    "Then the next point for descent should be:\n",
    "\n",
    "$ f(x_{n+1}) < f(x_n) $. The strict equality means that the descent method will never choose a new point that is the same level as the point it currently has.\n",
    "\n",
    "#### Level sets\n",
    "\n",
    "This is similar to the idea of a level curve on a map. It is the set of pairs, three tuples etc, that return the same function value.\n",
    "\n",
    "It's equivalent to cutting through a function with a plane that has reduced dimension by one dimension.\n",
    "\n",
    "If you were to look down onto the level set, it would look like this:\n",
    "\n",
    "![level set](Screenshot_2023-08-07_17-21-11.png)\n",
    "\n",
    "#### Choosing the gradient vector\n",
    "\n",
    "The gradient vector is the direction that maximises the directional derivative.\n",
    "\n",
    "Suppose $ ||\\hat v|| = 1 $, is the direction vector.\n",
    "\n",
    "The directional derivative (this is covered in /vectorCalculus) is:\n",
    "\n",
    "$ \\nabla f(\\bar x, \\bar y) \\cdot \\hat v = || \\nabla f(\\bar x, \\bar y) ||\\cdot ||\\hat v|| \\cdot \\cos \\theta $\n",
    "\n",
    "And since $ \\hat v = 1 $ then we can maximise the function by choosing $ \\cos \\theta = 1 $ and this washes out as:\n",
    "\n",
    "$  \\nabla f(\\bar x, \\bar y) \\cdot \\hat v = || \\nabla f(\\bar x, \\bar y) ||\\cdot ||\\hat v|| \\cdot \\cos \\theta = || f(\\bar x, \\bar y) || \\cdot 1 \\cdot 1 $\n",
    " \n",
    "So it's just this, where $ \\theta = 0 $\n",
    "\n",
    "$  \\nabla f(\\bar x, \\bar y) \\cdot \\hat v = || \\nabla f(\\bar x, \\bar y) || $\n",
    "\n",
    "$ \\hat v = \\large \\frac{\\nabla f(\\bar x, \\bar y)}{ || \\nabla f(\\bar x, \\bar y) || } $\n",
    "\n",
    "#### Finding the gradient of a level curve\n",
    "\n",
    "We can do this by parameterising the function $ f(x, y) \\to f(x(t), y(t))  $\n",
    "\n",
    "Then you can use the chain rule with partial derivatives:\n",
    "\n",
    "![partials in chain](Screenshot_2023-08-07_22-23-23.png)\n",
    "\n",
    "![dot prod form](Screenshot_2023-08-07_22-25-43.png)\n",
    "\n",
    "Since the dot product is equal to 0 then we know the two parts of the RHS are orthogonal.\n",
    "\n",
    "This part: $ \\frac{dx(\\bar t)}{dt}i + \\frac{dy(\\bar t)}{dt}j $ is tangent to the level curve, so that means the gradient orthogonal to the curve.\n",
    "\n",
    "If we're doing a minimisation problem, then at the point $ x_n $ the direction of descent is:\n",
    "\n",
    "$ d_n = - \\nabla f(x_n) $\n",
    "\n",
    "Where $ n $ is the index of steps taken.\n",
    "\n",
    "Recall that the step length, $ t $, is getting smaller. It is characterised by the Amijo Goldstein Condition. All together it looks like this:\n",
    "\n",
    "![descent](Screenshot_2023-08-07_22-37-56.png)\n",
    "\n",
    "The variable $ t $ is the exact step size to get to the next level curve.\n",
    "\n",
    "#### Descent direction matrix\n",
    "\n",
    "I have skipped over a lot of theory here to get assessments done, but, we can find the direction\n",
    "\n",
    "$ d = \\left(\\nabla^2 f(x)\\right)^{-1} \\cdot \\nabla f(x) $\n",
    "\n",
    "Where $ \\nabla^2 f(x) $ is the hessian of a function and it is positive definite.\n",
    "\n",
    "Then:\n",
    "\n",
    "$ \\nabla f(x) \\cdot d = -\\nabla f(x)^T \\cdot \\left(\\nabla^2 f(x)\\right)^{-1} \\cdot \\nabla f(x) $\n",
    "\n",
    "Suppose instead of the hessian matrix, we choose another matrix called $ D_k $\n",
    "\n",
    "Then if:\n",
    "\n",
    "$ D_k = I $, the identity matrix, then you get the Steepest Descent method.\n",
    "\n",
    "If:\n",
    "\n",
    "$ D_k = \\nabla^2 f(x_k)^{-1} $ then this is Newton's Method.\n",
    "\n",
    "![example with newton's method](Screenshot_2023-08-13_22-52-12.png)\n",
    "\n",
    "![Screenshot_2023-08-13_22-59-54.png](Screenshot_2023-08-13_22-59-54.png)\n",
    "\n",
    "![local min reached](Screenshot_2023-08-13_23-02-08.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccb26b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "6.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "994c48b1",
   "metadata": {},
   "source": [
    "### Approximate line search\n",
    "\n",
    "If you were to take hyperplane of a 3d space, with two independent variables, then you would have a point somewhere that is a local minimum or maximum.\n",
    "\n",
    "In $ \\mathbb{R}^n $, then this minimum is $ \\bar x = (\\bar x_1, \\bar x_2, \\bar x_3, ... , \\bar x_n) $ \n",
    "\n",
    "A change to the position is:\n",
    "\n",
    "$ d = (d_1, d_2, d_3, ..., d_n ) $\n",
    "\n",
    "We will know that $ \\bar x $ truly is a minimum if:\n",
    "\n",
    "$ d + \\bar x \\ge \\bar x $\n",
    "\n",
    "Otherwise, if $ \\bar x $ is not a minimum then there exists a direction $ (d_1, d_2, d_3, ..., d_n ) $\n",
    "\n",
    "Where $ d + \\bar x < \\bar x $\n",
    "\n",
    "Then this is the direction of descent.\n",
    "\n",
    "![conceptOfMinimum](Screenshot_2023-08-07_14-03-18.png)\n",
    "\n",
    "Where $ t $ is a scalar variable, that you would want to decrease as we get closer to some optimum.\n",
    "\n",
    "The exact line search is computationally expensive and generally not a good idea.\n",
    "\n",
    "Since the above is inefficient and really hard then it is more efficient to consider a single approximation only when we get close to the optimum.\n",
    "\n",
    "We need to determine the step length for point $ k $\n",
    "\n",
    "$ f(x_k + td_k) $\n",
    "\n",
    "To differentiate a function of one variable, in n-space, revisit the notebook `/vectorCalculus/Directional derivatives, gradient and angle (cosine rule).ipynb`\n",
    "\n",
    "We need to take the chain rule for partial derivatives.\n",
    "\n",
    "![gradient](Screenshot_2023-08-07_14-45-40.png)\n",
    "\n",
    "#### Inexact line search\n",
    "\n",
    "WHAT IS THIS FUNCTION AGAIN?\n",
    "\n",
    "##### WHAT ARE THE CONDITIONS\n",
    "\n",
    "THERE ARE TWO, WHAT ARE THEY?\n",
    "\n",
    "#### Deciding on a line step (Amijo Rule)\n",
    "\n",
    "This is computationally better than just using many small steps. We can use a larger step if we can for performance reasons.\n",
    "\n",
    "You choose a gradient at the origin $ g'(0) $, and then you draw another $ \\sigma g'(0) $\n",
    "\n",
    "The gradient at the origin, $ g'(0) $, will be below the function but the gradient $ \\sigma g'(0) $ can be above. \n",
    "\n",
    "$ \\sigma $ is positive so the effect of this is to make $ g'(0) $ less negative.\n",
    "\n",
    "Then there are small values of $ t $ that fall above $ g'(0) $ and below $ \\sigma g'(0) $\n",
    "\n",
    "Then we define another that is $ \\mu g'(0) $, and we scale it back until it's a tangent to the function. After this point, the gradient becomes less negative.\n",
    "\n",
    "$ g'(t) \\ge \\mu g'(0) $\n",
    "\n",
    "You must satisfy the wolf condition so the process doesn't stall.\n",
    "\n",
    "HOW DO WE CHOOSE t\n",
    "\n",
    "If you fall in the interval, you have both conditions satisfied\n",
    "\n",
    "You keep moving the low up, and the high one down, and take he midpoint until you land in an interval sunch that both the conditions are reached\n",
    "\n",
    "#### Deciding on direction\n",
    "\n",
    "The gradient vector is always expressed in the direction of maximal increase\n",
    "\n",
    "First option is to take the direction negative sign to gradient\n",
    "\n",
    "This is a descent method, the most basic, called steepest descent\n",
    "\n",
    "Stochastic steepest descent is the engine for a lot of machine learning\n",
    "\n",
    "__Every step, you calculate the descent direction as being the negative gradient_\n",
    "\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\\n",
    "\\.\n",
    "\n",
    "I lose concentration on this but continue at https://rmit.instructure.com/courses/107047/pages/week-4-learning-materials-slash-activities?module_item_id=5326821 10:12\n",
    "\n",
    "\n",
    "#### Optimality conditions (when to stop)\n",
    "\n",
    "To decide when to stop, consider the first derivative. It is a curve. It is the rate of change.\n",
    "\n",
    "When will this rate of change stop changing? Take the second derivative and check when its equality, or inequality, with 0.\n",
    "\n",
    "You have a stationary point if the gradient is 0, and to evaluate it then you need the second derivative. You need to look at the sign of the second derivative.\n",
    "\n",
    "### Example\n",
    "\n",
    "$ f(x, y) = x^2 - 2xy + 2y^2 - 4x $\n",
    "\n",
    "$ \\Delta f(x, y) = 2x - 27 - 4, -2x +4y) = (0,0) $\n",
    "\n",
    "For this to be true, $ (x, y) = (4, 2) $\n",
    "\n",
    "Then we can find if the point is positive definite by finding the determinant.\n",
    "\n",
    "### Eigenvalues look at the curvature of a surface\n",
    "\n",
    "If you have two eigenvalues, if one is positive and one is negative then you have a saddle point.\n",
    "\n",
    "In 2d this is because an eigenvalue multiplied by another can only be negative overall if one is negative and one is positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2178c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660945f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "6.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

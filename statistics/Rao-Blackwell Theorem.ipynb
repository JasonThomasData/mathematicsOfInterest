{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e882e79",
   "metadata": {},
   "source": [
    "### Rao-Blackwell Theorem\n",
    "\n",
    "The Cramer-Rao inequality provides a lower bound for an unbiased estimator.\n",
    "\n",
    "The theorem in the title shows that we can obtain a better unbiased estimator from two estimators:\n",
    "- One is unbiased for any function of $ \\theta $ (the one we start with, I think)\n",
    "- One is a sufficient statistic for $ \\theta $\n",
    "\n",
    "The result is that the new estimator has a variance no larger than the unbiased estimator. I think this means it might be better but at least it's not worse.\n",
    "\n",
    "##### Example\n",
    "\n",
    "Let $ X $ be a random sample. \n",
    "\n",
    "$ X = (X_1, X_2, X_3, ... X_n) $\n",
    "\n",
    "The population has pdf $ f(x; \\theta) $\n",
    "\n",
    "Let:\n",
    "\n",
    "$ T_1(X) $ be a sufficient statistic for $ \\theta $\n",
    "\n",
    "$ T_2(X) $ be an unbiased estimator of $ \\tau(\\theta) $ where $ \\tau $ is any function of $ \\theta $.\n",
    "\n",
    "Then the estimator defined by:\n",
    "\n",
    "$ \\phi(y) = E(T_2 | T_1 = y) $\n",
    "\n",
    "is an unbiased estimator of $ \\tau(\\theta) $\n",
    "\n",
    "In other words, this means:\n",
    "\n",
    "$ E(\\phi) = \\tau(\\theta) $\n",
    "\n",
    "$ Var(\\phi) \\le Var(T_2) $\n",
    "\n",
    "#### Proof for the Rao-Blackwell Theorem\n",
    "\n",
    "As above, let:\n",
    "\n",
    "$ T_1(X) $ be a sufficient statistic for $ \\theta $\n",
    "\n",
    "$ T_2(X) $ be an unbiased estimator of $ \\tau(\\theta) $ where $ \\tau $ is any function of $ \\theta $.\n",
    "\n",
    "$ \\phi(y) = E(T_2 | T_1 = y) $\n",
    "\n",
    "To interpret the above function, $ \\phi(y) $ is a function of the sufficient statistic $ T_1=y $.\n",
    "\n",
    "An important result concerns expected values:\n",
    "\n",
    "$ E(X) = E(E(X|Y)) $\n",
    "\n",
    "This is because when you find $ E(X|Y) $, and if the variables are continuous, then essentially integrate out the $ Y $ part. If the variables are discrete it's similiar but it's a summation.\n",
    "\n",
    "We can leverage this to say that since $ \\phi(y) = E(T_2 | T_1 = y) $ then:\n",
    "\n",
    "$ E(T_2) = E(E(T_2 | T_1)) = E(\\phi) $\n",
    "\n",
    "Another important result is for conditional variance\n",
    "\n",
    "Below, it's not really obvious as to the reason. It's covered in Lecture 3(II)a.\n",
    "\n",
    "$ Var(T_2) = E(Var(T_2 | T_1)) + Var(E(T_2 | T_1)) $\n",
    "\n",
    "But recall that $ \\phi = E(T_2 | T_1) $, so:\n",
    "\n",
    "$ Var(T_2) = E(Var(T_2 | T_1)) + Var(\\phi) $\n",
    "\n",
    "The sum-of-squares definition for variance means variance is always positive or $ 0 $.\n",
    "\n",
    "Therefore, the above equality requires:\n",
    "\n",
    "$ Var(T_2) \\ge Var(\\phi) $\n",
    "\n",
    "The result means that the variance of the unbiased estimator is at best as good as the variance for the estimator that combines $ T_1, T_2 $\n",
    "\n",
    "To be an estimator, $ \\phi(y) $ must not be a function of $ \\theta $.\n",
    "\n",
    "Recall: $ \\phi(y) = E(T_2 | T_1 = y) $\n",
    "\n",
    "$ T_1 $ is said to be a sufficient statistic, meaning that $ T_1 $ doesn't depend on $ \\theta $.\n",
    "\n",
    "I don't understand this part:\n",
    "\n",
    "We use this conditional distribution to compute $ E(T_2|T_1) $, and it is independent of $ \\theta $. Therefore $ \\phi $ doesn't contain $ \\theta $.\n",
    "\n",
    "DO AN EXAMPLE MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1311e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

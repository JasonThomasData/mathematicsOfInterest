{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62667a83",
   "metadata": {},
   "source": [
    "### Terminology use in inference\n",
    "\n",
    "#### Random Variable (r.v.)\n",
    "\n",
    "$ S = \\{s_1, s_2, ..., s_n \\} $\n",
    "\n",
    "Where $ n $ is the number of simple events, then $ S $ is the entire sample space.\n",
    "\n",
    "A random variable is:\n",
    "\n",
    "$ X : S \\to \\mathbb{R} $\n",
    "\n",
    "Then $ X $ maps a simple event to a real value.\n",
    "\n",
    "Then a certain mapping of a simple event is $ x_i = X(s_i) $\n",
    "\n",
    "A random variable cannot be certain.\n",
    "\n",
    "#### Discrete r.v.\n",
    "\n",
    "An r.v. is discrete if it assumes a non-countable set of values.\n",
    "\n",
    "#### Continuous r.v.\n",
    "\n",
    "An r.v. is continuous if it assumes a finite or countably infinite number of possible values. This is usually anything we measure.\n",
    "\n",
    "Since the values can't be counted then the probability that the r.v. takes on a particular value is always 0.\n",
    "\n",
    "$ P(X = a) = 0 $\n",
    "\n",
    "However the probability that it lies within an interval can be non-zero.\n",
    "\n",
    "And these are all equivalent:\n",
    "\n",
    "$ P(a \\le X \\le b) $\n",
    "\n",
    "$ P(a < X \\le b) $\n",
    "\n",
    "$ P(a \\le X < b) $\n",
    "\n",
    "$ P(a < X < b) $\n",
    "\n",
    "But this fact is generally not true for discrete r.v.\n",
    "\n",
    "#### Random Variable (distributed)\n",
    "\n",
    "If a random variable $ X $ is distributed by a certain distribution, $ D $, then:\n",
    "\n",
    "$ X \\sim D $\n",
    "\n",
    "#### Probability Mass Function (pmf)\n",
    "\n",
    "Concerning a discrete r.v.\n",
    "\n",
    "$ p(x) = P(X = x) $\n",
    "\n",
    "|      |     |     |     |     | \n",
    "|------|-----|-----|-----|-----|\n",
    "| X    | x_1 | x_2 | ... | x_n |\n",
    "| P(x) | p_1 | p_2 | ... | p_n |\n",
    "\n",
    "In other words, the probability that $ X $ takes the value $ x $.\n",
    "\n",
    "This is not the same as how we consider probabilities for continuous random variables.\n",
    "\n",
    "The pmf has these properties:\n",
    "\n",
    "- $ p(x) \\ge 0 $ for each $ x $\n",
    "\n",
    "- The sum of all $ p(x) = 1 $\n",
    "\n",
    "#### Probability Density Function (pdf)\n",
    "\n",
    "Concerning a continuous r.v.\n",
    "\n",
    "_Denoted by $ f(x) $ as opposed to $ F(x) $ for the CDF._\n",
    "\n",
    "To determine the probability that a r.v. is within (a, b) then we calculate the area under the curve of the pdf.\n",
    "\n",
    "$ (a \\le X \\le b) = \\int_a^b f(x) dx $\n",
    "\n",
    "Where $ f(x) $ is the pdf.\n",
    "\n",
    "_A pdf must have these properties:_\n",
    "\n",
    "$ f(x) \\ge 0 $ for all $ x $\n",
    "\n",
    "$ \\int_\\infty^\\infty f(x) dx = 1 $ \n",
    "\n",
    "So we can intuitively think of this as the pdf having infinite tails but not having $ f(x) \\ge 0 $ for entire interval. For some pdfs the range of integration is much smaller than the real number line.\n",
    "\n",
    "#### Sample density histogram (sdh)\n",
    "\n",
    "We can approximate the PDF by using a sample density histogram. This is the frequencies of results on the Y axis and its spread-outness should give you an idea of the probability density.\n",
    "\n",
    "To approximate the pdf, we should multiply a histogram bar height by its width (difference between intervals of x) to get an approximation for the area under the curve.\n",
    "\n",
    "#### Cumulative Distribution Function (cdf) (DISCRETE)\n",
    "\n",
    "$ F(x) = P(X \\le x) $\n",
    "\n",
    "In other words, it's the probability that a random event $ X $ (CAN IT BE DISCRETE?) is less than or equal to $ x $.\n",
    "\n",
    "The graph of the cdf should be a graph, where each $ p_i $ is the value of a step towards the upper limit of 1.\n",
    "\n",
    "Intuitively, we add up the discrete values.\n",
    "\n",
    "#### Cumulative Distribution Function (cdf) (CONTINUOUS)\n",
    "\n",
    "_Denoted by $ F(x) $ as opposed to $ f(x) $ for the PDF._\n",
    "\n",
    "$ F(x) = P(X \\le x) $\n",
    "\n",
    "And therefore it's conceptually the same as for discrete r.v. except for how we add up the values:\n",
    "\n",
    "$ F(x) = \\int_{-\\infty}^x f(t) dt $\n",
    "\n",
    "$ \\frac{d}{dx}(F(x)) = f(t) $\n",
    "\n",
    "#### Relationship between a pmf and its cdf\n",
    "\n",
    "_Need to check, this is for discrete r.v. only, right?_\n",
    "\n",
    "Given two fixed points $ x_1 < x_2 $\n",
    "\n",
    "$ P(x_1 < X \\le x_2) = F(x_2) - F(x_1) $\n",
    "\n",
    "Think about this one above first. Then:\n",
    "\n",
    "$ P(x_1 < X < x_2) = F(x_2 -) - F(x_1) $\n",
    "\n",
    "$ P(x_1 \\le X \\le x_2) = F(x_2) - F(x_1 -) $\n",
    "\n",
    "$ P(x_1 \\le X < x_2) = F(x_2 -) - F(x_1 -) $\n",
    "\n",
    "Where $ F(a -) $ refers to the limit approaching from the left.\n",
    "\n",
    "#### Relationship between a pdf and its cdf \n",
    "\n",
    "$ P(a < X < b) = F(b) - F(a) = \\int_a^b f(x) dx $\n",
    "\n",
    "#### Mean\n",
    "\n",
    "This is typically the mean taken from the sample. We compare this to the expected value.\n",
    "\n",
    "#### Expected value (discrete r.v.)\n",
    "\n",
    "\"mean of the random variable\" or $ \\mu $\n",
    "\n",
    "This is the mean of a probability distribution. This is what we would expect to see before any data is collected.\n",
    "\n",
    "Let:\n",
    "\n",
    "$ X $ be a discrete r.v.\n",
    "\n",
    "$ p(x) $ is the probability mass function of $ X $\n",
    "\n",
    "$ \\mu = E(X) = \\sum_i x_i p(x_i) = \\sum_i x_i p_i $\n",
    "\n",
    "For example, if there is a game where you could win or lose then the payoffs are:\n",
    "\n",
    "| result | p   |\n",
    "|--------|-----|\n",
    "| +1     | 0.2 |\n",
    "| +2     | 0.3 |\n",
    "| -2     | 0.2 |\n",
    "| -1     | 0.3 |\n",
    "\n",
    "Then:\n",
    "\n",
    "$ E(X) = (1)(0.2) + (2)(0.3) + (-2)(0.2) + (-1)(0.3) $\n",
    "\n",
    "$ E(X) = 0.2 + 0.6 - 0.4 - 0.3 = 0.1 $ is the expected payoff\n",
    "\n",
    "#### Expected value (continuous r.v.)\n",
    "\n",
    "\"mean of the random variable\" or $ \\mu $\n",
    "\n",
    "$ X $ be a discrete r.v.\n",
    "\n",
    "$ f(x) $ is the probability density function of $ X $\n",
    "\n",
    "\n",
    "$ \\mu = E(X) = \\int_{-\\infty}^\\infty x f(x) dx $\n",
    "\n",
    "#### Expected value with a constant\n",
    "\n",
    "For any random variable $ X $ and a constant $ a $\n",
    "\n",
    "$ E(X + a) = E(X) + a $\n",
    "\n",
    "$ E(Xa) = aE(X) $\n",
    "\n",
    "#### Expected values and linear combinations\n",
    "\n",
    "If you have sequences:\n",
    "\n",
    "$ \\{ X_1, X_2, ..., X_n \\} $ random variables\n",
    "\n",
    "$ \\{ a_1, a_2, ..., a_n \\} $ constantes\n",
    "\n",
    "Then:\n",
    "\n",
    "$ E(X) = E(a_1 X_1 + a_2 X_2 + ... + a_n X_n) =  a_1E(X_1) + a_2E(X_2) + ... + a_nE(X_n) ) $\n",
    "\n",
    "Then $ E(X) $ is the summation of all random variables, but $ X $ is not a variable itself, I don't think.\n",
    "\n",
    "#### Variance of a discrete r.v.\n",
    "\n",
    "This is the measure of dispersion of $ X $ from $ E(X) $\n",
    "\n",
    "This is:\n",
    "\n",
    "$ \\sigma^2 = Var(X) = E((X - \\mu)^2) = \\sum_i (X - \\mu)^2 p(x_i) $\n",
    "\n",
    "#### Variance of a continuous r.v.\n",
    "\n",
    "$ Var(X) = E((X - \\mu)^2) = \\int_{-\\infty}^\\infty (X - \\mu)^2 f(x) dx $\n",
    "\n",
    "$ \\sigma^2 = \\int_{-\\infty}^\\infty x^2 f(x) dx - \\mu^2$\n",
    "\n",
    "#### Variance with a constant\n",
    "\n",
    "For any random variable $ X $ and a constant $ a $\n",
    "\n",
    "$ Var(X + a) = Var(X) $\n",
    "\n",
    "$ Var(Xa) = a^2 Var(X) $\n",
    "\n",
    "If you have sequences:\n",
    "\n",
    "$ \\{ X_1, X_2, ..., X_n \\} $ random variables\n",
    "\n",
    "$ \\{ a_1, a_2, ..., a_n \\} $ constantes\n",
    "\n",
    "Then:\n",
    "\n",
    "$ Var(a_1 X_1 + a_2 X_2 + ... + a_n X_n) =  a_1^2 Var(X_1) + a_2^2 Var(X_2) + ... + a_n^2 Var(X_n) ) $\n",
    "\n",
    "#### Variance of a sum of two random variables\n",
    "\n",
    "Let:\n",
    "\n",
    "$ X, Y $ be random variables.\n",
    "\n",
    "$ Var(X, Y) = Var(X) + Var(Y) + 2Cov(X,Y) $\n",
    "\n",
    "If $X, Y$ are independent then $ Cov(X, Y) = 0 $\n",
    "\n",
    "Then:\n",
    "\n",
    "$ Var(X, Y) = Var(X) + Var(Y) $\n",
    "\n",
    "#### Standard deviation\n",
    "\n",
    "$ \\sigma = \\sqrt{Var(X)} $\n",
    "\n",
    "\n",
    "#### Expected value of an arbitrary function (discrete r.v.)\n",
    "\n",
    "$ E(g(X)) = \\sum_i g(x_i) p(x_i) $\n",
    "\n",
    "#### Expected value of an arbitrary function (continuous r.v.)\n",
    "\n",
    "$ E(g(X)) = \\int_{-\\infty}^\\infty g(x) f(x) dx $\n",
    "\n",
    "#### Skewness (discrete r.v.)\n",
    "\n",
    "Using the result above, we can use this to find the skewness of a pmf.\n",
    "\n",
    "$ g(x) = \\frac{(x-\\mu)^3}{\\sigma^3} $\n",
    "\n",
    "$ \\gamma = E(g(X)) = \\sum_i g(x_i) p(x_i) $\n",
    "\n",
    "$ \\gamma = \\sum_i \\frac{(x-\\mu)^3}{\\sigma^3} p(x_i) $\n",
    "\n",
    "$ \\gamma < 0 $ indicates the pmf has a negative skew.\n",
    "\n",
    "$ \\gamma = 0 $ indicates the pmf has no skew (symmetric).\n",
    "\n",
    "$ \\gamma > 0 $ indicates the pmf has a positive skew.\n",
    "\n",
    "#### Skewness (continuous r.v.)\n",
    "\n",
    "Using the $ E(X) $ from above, we can use this to find the skewness of a pdf.\n",
    "\n",
    "$ g(x) = \\frac{(x-\\mu)^3}{\\sigma^3} $\n",
    "\n",
    "This is the same as for the discrete r.v. except,\n",
    "\n",
    "$ \\gamma = E(g(X)) = \\frac{\\int_{-\\infty}^\\infty (x-\\mu)^3 f(x) dx}{\\sigma^3} $\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "Let: \n",
    "\n",
    "$ X, Y $ be two random variables\n",
    "\n",
    "$ \\mu_X, \\mu_Y $ be their mean respectively, and here we mean the expected values.\n",
    "\n",
    "$ Cov(X, Y) = E((X - \\mu_X)(Y - \\mu_Y)) $\n",
    "\n",
    "If $ X, Y $ are independent then $ Cov(X, Y) = 0 $\n",
    "\n",
    "But:\n",
    "\n",
    "$ Cov(X, Y) = 0 $ does not imply that $ X, Y $ are independent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "6.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

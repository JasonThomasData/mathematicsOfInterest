{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73490a09",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Consider a dataset that cannot be made to fit exactly onto a line.\n",
    "\n",
    "How can you find a line of best fit?\n",
    "\n",
    "The geometric intuition, for an independent variable a dependent variable, is a line through a 2d linear space. The line doesn't have to pass through the origin. It must have a y-intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6933252",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$ \\beta_0 $ be the y intercept \n",
    "\n",
    "$ \\beta_1 $ be the gradient \n",
    "\n",
    "$ e_i $ be the error. AKA the distance of the line (introduced below) away from a data point at $ i $ \n",
    "\n",
    "A line to fit this data can be described as:\n",
    "\n",
    "$ y_i = \\beta_0 + \\beta_1 x_i + e_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3795974",
   "metadata": {},
   "source": [
    "If the goal is to find $ \\beta_0, \\beta_1 $ while minimising $ e_i $\n",
    "\n",
    "Then:\n",
    "\n",
    "$ \\normalsize e_i = y_i - (\\beta_0 + \\beta_1 x_i) $\n",
    "\n",
    "Rather than just one $ e_i $, we want to find the values for all of those, so we should write it as:\n",
    "\n",
    "$ \\normalsize e = \\sum_{i=1}^n y_i - (\\beta_0 + \\beta_1 x_i) $\n",
    "\n",
    "But this poses a new problem: terms may cancel each other out (data above and below the line). Therefore, let's compute the sum of squares:\n",
    "\n",
    "$ \\normalsize e = \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 $\n",
    "\n",
    "With two variables to find, we can use partial derivatives.\n",
    "\n",
    "First, find the derivative in terms of $ \\beta_0 $ and treat $ \\beta_1 $ as a constant.\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 = 0 $\n",
    "\n",
    "Here, the partial derivative is equal to 0 because the purpose is to minimise the loss function $ e $, and the loss function's minima will mean the rate of change, aka the derivative, is 0 at that minima.\n",
    "\n",
    "Using the chain rule, this becomes:\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n 2 \\left[ y_i - \\beta_0 - \\beta_1 x_i \\right] (-1) = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n -2 \\left[ y_i - \\beta_0 - \\beta_1 x_i \\right] = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n \\frac{\\left[ y_i - \\beta_0 - \\beta_1 x_i \\right]}{-2} = \\frac{0}{-2} $\n",
    "\n",
    "Since this doesn't make sense, let's ignore the -2\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\beta_0 - \\sum_{i=1}^n \\beta_1 x_i = 0 $\n",
    "\n",
    "$ \\beta_0 $ here has a single value. Therefore it is $ n\\beta_0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n y_i - n\\beta_0 -  \\beta_1 \\sum_{i=1}^n x_i = 0 $\n",
    "\n",
    "We now need to find the other partial derivative\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n 2 \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] (-x_i) = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n -2x_i \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] = 0 $\n",
    "\n",
    "For the same reason as last time, after dividing by 2, this becomes\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n x_i \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] = 0 $\n",
    "\n",
    "$  \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n y_i x_i - \\beta_0 \\sum_{i=1}^n x_i -  \\beta_1 \\sum_{i=1}^n x_i^2 = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b046e",
   "metadata": {},
   "source": [
    "Now you have two linear equations and can solve this as simultaneous equations. We can use the elimination method to derive the $ \\beta_0, \\beta_1 $ separately.\n",
    "\n",
    "$ [1] : \\sum_{i=1}^n y_i - n\\beta_0 -  \\beta_1 \\sum_{i=1}^n x_i = 0 $\n",
    "\n",
    "$ [2] : \\sum_{i=1}^n y_i x_i - \\beta_0 \\sum_{i=1}^n x_i -  \\beta_1 \\sum_{i=1}^n x_i^2 = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e1549",
   "metadata": {},
   "source": [
    "#### TODO - show how to solve equations using elimination of simultaneous equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea006b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0 =  4.6308\n",
      "beta_1 =  0.98320\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [1:20];\n",
    "n = length(x);\n",
    "y = rand(1,20).*10 + x;\n",
    "plot(x, y, \"o\");\n",
    "\n",
    "beta_1 = (n * dot(x,y) - sum(x)*sum(y))/(n * sum(x.^2) - sum(x)^2);\n",
    "beta_0 = (1/n)* sum(y) - beta_1 * (1/n) * sum(x);\n",
    "\n",
    "regressionLine = beta_0 + x*beta_1;\n",
    "\n",
    "line(x,regressionLine);\n",
    "\n",
    "beta_0\n",
    "beta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7834bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_1 =  0.98320\n",
      "b_0 =  4.6308\n"
     ]
    }
   ],
   "source": [
    "b_1 = cov(x, y)/var(x)\n",
    "b_0 = mean(y) - b_1 * mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ac00e",
   "metadata": {},
   "source": [
    "### In terms of covariance and variance (probably more intuitive)\n",
    "\n",
    "For reasons I don't understand, you can calculate the slope of the line with:\n",
    "\n",
    "$ b_0 = \\large \\frac{cov(x,y)}{var(x)} $\n",
    "\n",
    "Where\n",
    "\n",
    "$ n $ is the population, $ n-1 $ is for the sample. Octave uses $n-1$\n",
    "\n",
    "$ S_{xy} $ aka covariance:\n",
    "\n",
    "$ \\frac{\\sum{(x_i - \\bar x)} \\sum{(y_i - \\bar y)}}{n-1}$\n",
    "\n",
    "$ S_{xx} $ aka variance aka $ s^2 $ for a sample :\n",
    "\n",
    "$ \\frac{\\sum{(x_i - \\bar x)}^2}{n-1}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$ b_0 = \\large \\frac{cov(x,y)}{var(x)} $\n",
    "\n",
    "And since\n",
    "\n",
    "$ \\bar{y} = \\beta_0 + \\beta_1 \\bar{x} $\n",
    "\n",
    "Then:\n",
    "\n",
    "$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $\n",
    "\n",
    "### Analogous to the rise over the run\n",
    "\n",
    "This site - https://www.statology.org/covariance-vs-variance/ - describes:\n",
    "\n",
    "covariance, $ S_{xy} $: a measure of how changes in one variable are related to the changes in another variable.\n",
    "\n",
    "variance, $ S_{xx} $: how spread out the data are\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\frac{S_{xy}}{S_{xx}}$ means \"How we can understand changes in Y (the rise) in terms of how spread out the variable X is (the run)\". This seems somewhat like the rise/run formula for finding any linear gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0e2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

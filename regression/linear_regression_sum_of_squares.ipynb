{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73490a09",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Consider a dataset that cannot be made to fit exactly onto a line.\n",
    "\n",
    "How can you find a line of best fit?\n",
    "\n",
    "The geometric intuition, for an independent variable a dependent variable, is a line through a 2d linear space. The line doesn't have to pass through the origin. It must have a y-intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6933252",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$ \\beta_0 $ be the y intercept \n",
    "\n",
    "$ \\beta_1 $ be the gradient \n",
    "\n",
    "$ e_i $ be the error. AKA the distance of the line (introduced below) away from a data point at $ i $ \n",
    "\n",
    "A line to fit this data can be described as:\n",
    "\n",
    "$ y_i = \\beta_0 + \\beta_1 x_i + e_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3795974",
   "metadata": {},
   "source": [
    "If the goal is to find $ \\beta_0, \\beta_1 $ while minimising $ e_i $\n",
    "\n",
    "Then:\n",
    "\n",
    "$ \\normalsize e_i = y_i - (\\beta_0 + \\beta_1 x_i) $\n",
    "\n",
    "Rather than just one $ e_i $, we want to find the values for all of those, so we should write it as:\n",
    "\n",
    "$ \\normalsize e = \\sum_{i=1}^n y_i - (\\beta_0 + \\beta_1 x_i) $\n",
    "\n",
    "But this poses a new problem: terms may cancel each other out (data above and below the line). Therefore, let's compute the sum of squares:\n",
    "\n",
    "$ \\normalsize e = \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 $\n",
    "\n",
    "With two variables to find, we can use partial derivatives.\n",
    "\n",
    "First, find the derivative in terms of $ \\beta_0 $ and treat $ \\beta_1 $ as a constant.\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 = 0 $\n",
    "\n",
    "Here, the partial derivative is equal to 0 because the purpose is to minimise the loss function $ e $, and the loss function's minima will mean the rate of change, aka the derivative, is 0 at that minima.\n",
    "\n",
    "Using the chain rule, this becomes:\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n 2 \\left[ y_i - \\beta_0 - \\beta_1 x_i \\right] (-1) = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n -2 \\left[ y_i - \\beta_0 - \\beta_1 x_i \\right] = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n \\frac{\\left[ y_i - \\beta_0 - \\beta_1 x_i \\right]}{-2} = \\frac{0}{-2} $\n",
    "\n",
    "Since this doesn't make sense, let's ignore the -2\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\beta_0 - \\sum_{i=1}^n \\beta_1 x_i = 0 $\n",
    "\n",
    "$ \\beta_0 $ here has a single value. Therefore it is $ n\\beta_0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n y_i - n\\beta_0 -  \\beta_1 \\sum_{i=1}^n x_i = 0 $\n",
    "\n",
    "We now need to find the other partial derivative\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n 2 \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] (-x_i) = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n -2x_i \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] = 0 $\n",
    "\n",
    "For the same reason as last time, after dividing by 2, this becomes\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n x_i \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] = 0 $\n",
    "\n",
    "$  \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n y_i x_i - \\beta_0 \\sum_{i=1}^n x_i -  \\beta_1 \\sum_{i=1}^n x_i^2 = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b046e",
   "metadata": {},
   "source": [
    "Now you have two linear equations and can solve this as simultaneous equations. We can use the elimination method to derive the $ \\beta_0, \\beta_1 $ separately.\n",
    "\n",
    "$ [1] : \\sum_{i=1}^n y_i - n\\beta_0 -  \\beta_1 \\sum_{i=1}^n x_i = 0 $\n",
    "\n",
    "$ [2] : \\sum_{i=1}^n y_i x_i - \\beta_0 \\sum_{i=1}^n x_i -  \\beta_1 \\sum_{i=1}^n x_i^2 = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e1549",
   "metadata": {},
   "source": [
    "#### TODO - show how to solve equations using elimination of simultaneous equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea006b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0 =  3.2038\n",
      "beta_1 =  1.0618\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [1:10];\n",
    "n = length(X);\n",
    "Y = rand(1,10).*10 + X;\n",
    "plot(X, Y, \"o\");\n",
    "\n",
    "beta_1 = (n * dot(X,Y) - sum(X)*sum(Y))/(n * sum(X.^2) - sum(X)^2);\n",
    "beta_0 = (1/n)* sum(Y) - beta_1 * (1/n) * sum(X);\n",
    "\n",
    "regressionLine = beta_0 + X*beta_1;\n",
    "\n",
    "line(X,regressionLine);\n",
    "\n",
    "beta_0\n",
    "beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ac00e",
   "metadata": {},
   "source": [
    "### In terms of covariance and variance (probably more intuitive)\n",
    "\n",
    "You can calculate the slope of the line with:\n",
    "\n",
    "$ \\beta_1 = \\large \\frac{cov(x,y)}{var(x)} $\n",
    "\n",
    "Where\n",
    "\n",
    "$ n $ is the population, $ n-1 $ is for the sample. Octave uses $n-1$\n",
    "\n",
    "$ \\frac{S_{xy}}{n-1} $ aka covariance:\n",
    "\n",
    "$ \\frac{\\sum{(x_i - \\bar x)} \\sum{(y_i - \\bar y)}}{n-1}$\n",
    "\n",
    "$ \\frac{S_{xx}}{n-1} $ aka variance aka $ s^2 $ for a sample :\n",
    "\n",
    "$ \\frac{\\sum{(x_i - \\bar x)}^2}{n-1}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$ \\beta_1 = \\frac{cov(x,y)}{var(x)} = \\frac{S_{xy}}{n-1} / \\frac{S_{xx}}{n-1} = \\frac{S_{xy}}{S_{xx}} $\n",
    "\n",
    "And since\n",
    "\n",
    "$ \\bar{y} = \\beta_0 + \\beta_1 \\bar{x} $\n",
    "\n",
    "Then:\n",
    "\n",
    "$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7834bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_1 =  1.0618\n",
      "b_0 =  3.2038\n"
     ]
    }
   ],
   "source": [
    "b_1 = cov(X, Y)/var(X)\n",
    "b_0 = mean(Y) - b_1 * mean(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cc101",
   "metadata": {},
   "source": [
    "Is equivalent to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a69105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_1 =  1.0618\n",
      "b_0 =  3.2038\n"
     ]
    }
   ],
   "source": [
    "x_bar = mean(X);\n",
    "y_bar = mean(Y);\n",
    "\n",
    "S_xy = sum((X .- x_bar).*(Y .- y_bar));\n",
    "S_xx = sum((X .- x_bar).^2);\n",
    "\n",
    "b_1 = S_xy/S_xx\n",
    "b_0 = mean(Y) - b_1 * mean(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadee6cb",
   "metadata": {},
   "source": [
    "### Some intuition\n",
    "\n",
    "This site - https://www.statology.org/covariance-vs-variance/ - describes:\n",
    "\n",
    "covariance, $ \\frac{S_{xy}}{n-1} $: a measure of how changes in one variable are related to the changes in another variable. $ n-1 $ is for a sample.\n",
    "\n",
    "variance, $ S_{xx} $: how spread out the data are\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\frac{S_{xy}}{n-1} / \\frac{S_{xx}}{n-1} = \\frac{S_{xy}}{S_{xx}}$ means \"How we can understand changes in Y (the rise) in terms of how spread out the variable X is (the run)\". This seems somewhat like the rise/run formula for finding any linear gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de153b",
   "metadata": {},
   "source": [
    "### Matrix representation\n",
    "\n",
    "This is an approach for using matrix algebra. This can be extended for use with multiple regression.\n",
    "\n",
    "$ y' = (y_1, y_2, ... y_n ) $\n",
    "\n",
    "$ p $ : the number of variables (simple regression has just 1)\n",
    "\n",
    "$ \\beta = (\\beta_0, \\beta_1, \\beta_2 ... \\beta_p) $ : the first is $ \\beta_0 $, aka, the y intercept and the rest are for each of the regression gradients\n",
    "\n",
    "$ \\epsilon = (\\epsilon_1, \\epsilon_2 ... \\epsilon_n) $ the residuals\n",
    "\n",
    "$ X = \\left[\\begin{matrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ ... & ... & ... & ... & ... \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{matrix} \\right]$\n",
    "\n",
    "$ y = X\\beta + \\epsilon $\n",
    "\n",
    "$ Y = \\left[\\begin{matrix} 1 & x_{11} & x_{12} & ... & x_{1p} \\\\ 1 & x_{21} & x_{22} & ... & x_{2p} \\\\ ... & ... & ... & ... & ... \\\\ 1 & x_{n1} & x_{n2} & ... & x_{np} \\end{matrix} \\right] \n",
    "\\left[\\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ ... \\\\ \\beta_p \\end{matrix} \\right]\n",
    "+\n",
    "\\left[\\begin{matrix} \\epsilon_0 & \\epsilon_1 & \\epsilon_2 & ... & \\epsilon_n \\end{matrix} \\right]$\n",
    "\n",
    "If we make the naive assumption that $\\epsilon = {0} $ then we can solve for $ \\beta $\n",
    "\n",
    "That means that $ Y $ is a linear combination of the column vectors of $ X $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a73440c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    beta_0    beta_1    Y\n",
      "    1.0000    1.0000    1.8814\n",
      "    1.0000    2.0000    6.7001\n",
      "    1.0000    3.0000    4.7792\n",
      "    1.0000    4.0000    5.2143\n",
      "    1.0000    5.0000   14.7451\n",
      "    1.0000    6.0000    8.1116\n",
      "    1.0000    7.0000   15.9958\n",
      "    1.0000    8.0000    8.6385\n",
      "    1.0000    9.0000   12.6533\n",
      "    1.0000   10.0000   11.7162\n",
      "ans =\n",
      "\n",
      "   3.2038\n",
      "   1.0618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Toy example with just beta_0 and beta_1\n",
    "onesConstant = repmat([1], 1, 10); #Row of 1s, because this is the coefficient for y intercept.\n",
    "designMatrix = [onesConstant' X']; #X is the coefficient for the gradient aka beta_1\n",
    "augmentedMatrix = [designMatrix transpose(Y)];\n",
    "disp([\"    beta_0\", \"    beta_1\", \"    Y\"])\n",
    "disp(augmented_matrix)\n",
    "X_Matrix\\transpose(Y) #this is inconsistent, so this must use least squares method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c02b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

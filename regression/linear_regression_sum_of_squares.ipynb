{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73490a09",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Consider a dataset that cannot be made to fit exactly onto a line.\n",
    "\n",
    "How can you find a line of best fit?\n",
    "\n",
    "The geometric intuition, for an independent variable a dependent variable, is a line through a 2d linear space. The line doesn't have to pass through the origin. It must have a y-intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6933252",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$ \\beta_0 $ be the y intercept \n",
    "\n",
    "$ \\beta_1 $ be the gradient \n",
    "\n",
    "$ e_i $ be the error. AKA the distance of the line (introduced below) away from a data point at $ i $ \n",
    "\n",
    "A line to fit this data can be described as:\n",
    "\n",
    "$ y_i = \\beta_0 + \\beta_1 x_i + e_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3795974",
   "metadata": {},
   "source": [
    "If the goal is to find $ \\beta_0, \\beta_1 $ while minimising $ e_i $\n",
    "\n",
    "Then:\n",
    "\n",
    "$ \\normalsize e_i = y_i - (\\beta_0 + \\beta_1 x_i) $\n",
    "\n",
    "Rather than just one $ e_i $, we want to find the values for all of those, so we should write it as:\n",
    "\n",
    "$ \\normalsize e = \\sum_{i=1}^n y_i - (\\beta_0 + \\beta_1 x_i) $\n",
    "\n",
    "But this poses a new problem: terms may cancel each other out (data above and below the line). Therefore, let's compute the sum of squares:\n",
    "\n",
    "$ \\normalsize e = \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 $\n",
    "\n",
    "With two variables to find, we can use partial derivatives.\n",
    "\n",
    "First, find the derivative in terms of $ \\beta_0 $ and treat $ \\beta_1 $ as a constant.\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 = 0 $\n",
    "\n",
    "Here, the partial derivative is equal to 0 because the purpose is to minimise the loss function $ e $, and the loss function's minima will mean the rate of change, aka the derivative, is 0 at that minima.\n",
    "\n",
    "Using the chain rule, this becomes:\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n 2 \\left[ y_i - \\beta_0 - \\beta_1 x_i \\right] (-1) = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n -2 \\left[ y_i - \\beta_0 - \\beta_1 x_i \\right] = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n \\frac{\\left[ y_i - \\beta_0 - \\beta_1 x_i \\right]}{-2} = \\frac{0}{-2} $\n",
    "\n",
    "Since this doesn't make sense, let's ignore the -2\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n y_i - \\sum_{i=1}^n \\beta_0 - \\sum_{i=1}^n \\beta_1 x_i = 0 $\n",
    "\n",
    "$ \\beta_0 $ here has a single value. Therefore it is $ n\\beta_0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_0} \\normalsize \\sum_{i=1}^n y_i - n\\beta_0 -  \\beta_1 \\sum_{i=1}^n x_i = 0 $\n",
    "\n",
    "We now need to find the other partial derivative\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right]^2 = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n 2 \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] (-x_i) = 0 $\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n -2x_i \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] = 0 $\n",
    "\n",
    "For the same reason as last time, after dividing by 2, this becomes\n",
    "\n",
    "$ \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n x_i \\left[ y_i - \\beta_0 - \\beta_1 x_i) \\right] = 0 $\n",
    "\n",
    "$  \\large \\frac{\\partial}{\\partial \\beta_1} \\normalsize \\sum_{i=1}^n y_i x_i - \\beta_0 \\sum_{i=1}^n x_i -  \\beta_1 \\sum_{i=1}^n x_i^2 = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b046e",
   "metadata": {},
   "source": [
    "Now you have two linear equations and can solve this as simultaneous equations. We can use the elimination method to derive the $ \\beta_0, \\beta_1 $ separately.\n",
    "\n",
    "$ [1] : \\sum_{i=1}^n y_i - n\\beta_0 -  \\beta_1 \\sum_{i=1}^n x_i = 0 $\n",
    "\n",
    "$ [2] : \\sum_{i=1}^n y_i x_i - \\beta_0 \\sum_{i=1}^n x_i -  \\beta_1 \\sum_{i=1}^n x_i^2 = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e1549",
   "metadata": {},
   "source": [
    "#### TODO - show how to solve equations using elimination of simultaneous equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea006b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0 =  7.6704\n",
      "beta_1 =  0.81993\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [1:20];\n",
    "n = length(X);\n",
    "Y = rand(1,20).*10 + X;\n",
    "plot(X, Y, \"o\");\n",
    "\n",
    "beta_1 = (n * dot(X,Y) - sum(X)*sum(Y))/(n * sum(X.^2) - sum(X)^2);\n",
    "beta_0 = (1/n)* sum(Y) - beta_1 * (1/n) * sum(X);\n",
    "\n",
    "regressionLine = beta_0 + X*beta_1;\n",
    "\n",
    "line(X,regressionLine);\n",
    "\n",
    "beta_0\n",
    "beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ac00e",
   "metadata": {},
   "source": [
    "### In terms of covariance and variance (probably more intuitive)\n",
    "\n",
    "You can calculate the slope of the line with:\n",
    "\n",
    "$ \\beta_1 = \\large \\frac{cov(x,y)}{var(x)} $\n",
    "\n",
    "Where\n",
    "\n",
    "$ n $ is the population, $ n-1 $ is for the sample. Octave uses $n-1$\n",
    "\n",
    "$ \\frac{S_{xy}}{n-1} $ aka covariance:\n",
    "\n",
    "$ \\frac{\\sum{(x_i - \\bar x)} \\sum{(y_i - \\bar y)}}{n-1}$\n",
    "\n",
    "$ \\frac{S_{xx}}{n-1} $ aka variance aka $ s^2 $ for a sample :\n",
    "\n",
    "$ \\frac{\\sum{(x_i - \\bar x)}^2}{n-1}$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$ \\beta_1 = \\frac{cov(x,y)}{var(x)} = \\frac{S_{xy}}{n-1} / \\frac{S_{xx}}{n-1} = \\frac{S_{xy}}{S_{xx}} $\n",
    "\n",
    "And since\n",
    "\n",
    "$ \\bar{y} = \\beta_0 + \\beta_1 \\bar{x} $\n",
    "\n",
    "Then:\n",
    "\n",
    "$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b7834bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_1 =  0.81993\n",
      "b_0 =  7.6704\n"
     ]
    }
   ],
   "source": [
    "b_1 = cov(X, Y)/var(X)\n",
    "b_0 = mean(Y) - b_1 * mean(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905cc101",
   "metadata": {},
   "source": [
    "Is equivalent to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a69105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_1 =  0.81993\n",
      "b_0 =  7.6704\n"
     ]
    }
   ],
   "source": [
    "x_bar = mean(X);\n",
    "y_bar = mean(Y);\n",
    "\n",
    "S_xy = sum((X .- x_bar).*(Y .- y_bar));\n",
    "S_xx = sum((X .- x_bar).^2);\n",
    "\n",
    "b_1 = S_xy/S_xx\n",
    "b_0 = mean(Y) - b_1 * mean(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadee6cb",
   "metadata": {},
   "source": [
    "### Some intuition\n",
    "\n",
    "This site - https://www.statology.org/covariance-vs-variance/ - describes:\n",
    "\n",
    "covariance, $ \\frac{S_{xy}}{n-1} $: a measure of how changes in one variable are related to the changes in another variable. $ n-1 $ is for a sample.\n",
    "\n",
    "variance, $ S_{xx} $: how spread out the data are\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$\\frac{S_{xy}}{n-1} / \\frac{S_{xx}}{n-1} = \\frac{S_{xy}}{S_{xx}}$ means \"How we can understand changes in Y (the rise) in terms of how spread out the variable X is (the run)\". This seems somewhat like the rise/run formula for finding any linear gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185dd014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "5.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
